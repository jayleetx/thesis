```{r include = FALSE}
library(here)
library(purrr)
library(sf)
library(ggplot2)
library(patchwork)
```

# Methods and Structure {#methods}

Inevitably, the data format that would be ideal to conduct this research doesn't exist publicly (as it shouldn't - voter anonymity and such). The ideal model might be a logistic or other classification model that predicts whether a given voter has a ballot error or fills out the ballot incompletely, given their demographic qualities.

## Data Structure and Source

### Election-specific data

The data used comes from two main sources. From the San Francisco Elections Office <!-- Get an official title? -->, we have a cast ballot record of the election in question <!-- Put a date on this! --> (source?). This is presented by the city as two text files:
- The ballot image, a 45-character fixed-width file with fields corresponding to election, candidate, unique voter number (anonymized and disconnected from any voter registration ID), ranking, precinct, and other information. Each of these is encoded numerically, so each line of the file appears as a 45-digit number.
- The lookup table, an 83-character fixed-width file that defines the encodings used to create the numerical values in the ballot image.
<!-- Get the pictures that explain these! -->

In the ballot file, each voter is spread across three rows of data (one for each ranking). We use the `clean_ballot()` function from our `rcv` R package <!-- cite this somehow? idk --> to read in the data for easier manipulation. From this data we have the full ranked preference set of candidates (up to 3) from each voter.

Since the voter-level data is fully anonymized, we have no demographic information at the individual level. For any given voter, since the most identifying piece of information we have about them is their precinct, it is impossible to know any one voter's gender or ethnicity. To gain insight into these demographic trends, we instead aggregate up to the precinct level.

At the precinct level, we can now only study the rates of these ballot phenomena (as opposed to an individual's response). For example, in Precinct 1703, we may see 14% of voters undervote. <!-- Get a real stat to use for this --> Rather than building a classification model (error / no error), we can now instead build regression models with a numerical dependent variable <!-- Better word for this --> - the rate of ballot error. Moving up this level of abstraction does remove some granularity from the model (inference and prediction at the precinct level is less specific than at the individual level), but this is the least we can do and still be able to access demographic information.
<!-- Talk about the actual processes (and files) used to do this -->

### Demographic data

Edit this paragraph because it's not right - ACS, no MOE, etc.
Now that we have precinct-level rates, we need to obtain precinct-level demographic information in order to build a model. Our source of demographic data is the 2010 U.S. Census as part of the 2018 Census Planning Database (source? Also map source). While this data is slightly out of date (about 8 years), there is more certainty about the accuracy of the measurements collected. While a more temporally accurate data set like the yearly American Community Survey (ACS) could have been used, the error margins of this data set proved too large / complicated to be useful for this study.

One consideration with this is that the voting population is not always representative of the general population. It might be more informative to obtain demographics about the voting population specifically, rather than the entire population of each precinct. <!-- Find out how many people in each precinct voted. Should be some easy math once you find the population of the precincts. --> However, the only information available about voters is their age (from the county voter registration file), and the discontinuity introduced by using two different data sets for demographic information is more of an  "error" to me than using a less accurate (but universal) census data set.

We now encounter a problem. Since census regions (block groups, tracts) are set by the federal government, and election precincts are set by San Francisco County^[The city and county government are unified in this case, because the county comprises entirely of the city of San Francisco.], the regions don't line up nicely^[There's no inherent reason that they should, it's just unfortunate for this study.]. <!-- show a picture of this! You have two maps --> Given this mismatch, how do we obtain demographic estimates for our precincts?

Stated more generally: if we have a division of a geographic region and some set of properties on the divisions, how do we estimate measures of these properties for other possible divisions?

#### Areal Interpolation

One field of GIS (geographic information system) research that looks into this is called *areal interpolation*. The goal of areal interpolation is to take a variable distributed over a set of "source zones" and estimate its distribution over a set of "target zones" (Schroeder, 2007). These two zoning systems must overlap at least partially to get any estimate for the target zones (e.g. information about Oregon doesn't directly tell us anything about information in Washington, under any zoning system), but are incompatible in some way^[A set of "compatible" zoning systems would be something like the US Census' hierarchical systems: multiple blocks are combined to make a block group, multiple block groups are combined into a census tract, etc. Since the areas overlap neatly, you can directly add together certain numbers (like population) from block groups to get a very accurate estimate for the census tract.]. When this is not possible, however, areal interpolation can help get parameter estimates for these incompatible areas.

Soem common types of areal interpolation are:

- Areal weighting, using the assumption that populations are distributed uniformly on a region.

- Modified areal weighting, which creates a continuous map from region to region to better reflect changes in population density.

- Target density weighting, which uses extra information about the target zones, typically population density, to increase accuracy (Schroeder, 2007).

- Dasymetric mapping, which uses alternative data, also called *ancillary data* to produce a continuous estimate of population density (Sleeter and Gould, 2008). Examples of ancillary data include land cover information, parcel classifications, and street locations.

In this work we will use areal weighting for our interpolation. The other methods above, while typically more accurate are infeasible under the constraints of the research^[Mostly time limitations; see Conclusion section for further research ideas.]. The assumption of uniformity is not necessarily correct, but in an urban area such as San Francisco it is more accurate than an area like the entire state of California, with a large urban/rural spread. 

#### Areal weighting

Areal weighting first makes the assumption that populations are distributed uniformly over a space. If Region X has 100 people and we split X in half spatially, then we assume that 50 people are in each half. This also applies to sub-populations: if Region X has 20 non-White Hispanic people, then we assume each half has 10 non-white Hispanic people. This type of data, counts that can be divided into sub-regions, is called *spatially extensive* data. Extensive data is data that applies to an entire region, but not any given sub-region.

Conversely, data that applies to any given sub-region of a region is called *spatially intensive*. Properties like population density are spatially intensive under the uniformity assumption, because the ratio of population to area does not change upon examining a sub-region. Percentages are also spatially intensive - considering the sub-population as above, the percentage of non-White Hispanic people in Region X (20 out of 100, 20%) does not change when we look at one of the sub-regions (10 out of 50, 20%).

We will use the following example case to illustrate the areal interpolation process. Suppose regions $A$, $B$, $C$, and $D$ are the source regions (each taking up a quadrant of the square), and regions $X$, $Y$, and $Z$ are the target regions (each taking up a third of the square vertically). Further suppose that the boundaries of the source regions and target regions are fully coincident, that is $A \cup B \cup C \cup D = X \cup Y \cup Z$^[This example ignores the case where the covered regions are not coincident, which is a possibility in general. In this research we enforce coincidence in our data, however.].

```{r include=FALSE, fig.cap='Example regions for interpolation'}
source <- st_read(here('raw_data', 'source_zones')) %>%
  transmute(name = c('D','C','A','B'),
            lon = map_dbl(geometry, ~st_centroid(.x)[[1]]),
            lat = map_dbl(geometry, ~st_centroid(.x)[[2]]))
target <- st_read(here('raw_data', 'target_zones')) %>%
  transmute(name = c('Z','X','Y'),
            lon = map_dbl(geometry, ~st_centroid(.x)[[1]]),
            lat = map_dbl(geometry, ~st_centroid(.x)[[2]]))

source_plot <- ggplot(source) + geom_sf() + geom_text(aes(x = lon, y = lat, label = name)) + theme_void() + ggtitle('Source regions')

target_plot <- ggplot(target) + geom_sf() + geom_text(aes(x = lon, y = lat, label = name)) + theme_void() + ggtitle('Target regions')

source_plot + target_plot + plot_layout(ncol = 2)
```

In general, denote a source region by $S_i$ (over index set $I$) and a target region by $T_j$ (over index set $J$). For any region $R$, denote the area of $R$ by $Area(R)$, the measure of a given extensive property of $R$ by $x_R$, and the measure of a given intensive property of $R$ by $y_R$. These measures are known in the source regions, but unknown in the target regions (hence, the interpolation). Denote an estimate of a quantity with a carat, e.g. $\widehat{y_{R}}$.

Say we want to estimate $x_{T_j}$, the measure of the extensive property in region $T_j$. For all $j \in J$, we can estimate this property with
\[
\widehat{x_{T_j}} = \sum_{i \in I} \frac{Area(S_i \cap T_j)}{Area(S_i)} \cdot x_{S_i}
\]

For each source region, calculate the proportion of the region that lies inside the target region. These proportions are the weights to be multiplied by the source regions' properties. In the example case, consider target region $X$:
\[
\widehat{x_X} = \frac{Area(A \cap X)}{Area(A)} \cdot x_{A} + \frac{Area(B \cap X)}{Area(B)} \cdot x_{B} + 0 + 0 = \frac{2}{3}(x_A + x_B)
\]

Since $A$ and $B$ each have $2/3$ of their area inside target region $X$, we estimate that 2/3 of the extensive quantity $x_A$ is inside region $X$ (similarly for $x_B$). Adding these together gives us an estimate $\widehat{x_X}$.

For intensive properties, the process is slightly different. For all $j \in J$, we can estimate an intensive property $y_{T_j}$ with
\[
\widehat{y_{T_j}} = \sum_{i \in I} \frac{Area(S_i \cap T_j)}{Area(T_j)} \cdot x_{S_i}
\]

Since the intensive property isn't "divisible" within a region, we instead take a weighted average of the component source regions of the target region, where each component is weighted by the amount of the target region it takes up. Again considering target region $X$:
\[
\widehat{y_X} = \frac{Area(A \cap X)}{Area(X)} \cdot y_{A} + \frac{Area(B \cap X)}{Area(X)} \cdot y_{B} + 0 + 0 = \frac{1}{2}(y_A + y_B)
\]

Since $A$ and $B$ each take up half of target region $X$, each of them gets weighted by half before being added to get the estimate of $y_X$.

To validate the application of this method to the data at hand, we can visually compare the spatial distribution of a variable before the interpolation to its distribution after the interpolation.

```{r echo = FALSE, fig.width=6.5, fig.cap = 'Proportion of non-Hispanic Asian residents'}
knitr::include_graphics(here('img', 'interpolation_check.png'))
```

#### Interpolation - counts vs. percentages

One consideration in the data preparation stage was whether to use counts or percentages as input for the regression. The census data contains total population in a region, as well as a count and percentage for a given variable (say people between the ages of 25 and 44). The percentage can equivalently be calculated by dividing the count by the population. After performing the areal interpolation on the data, I ran this percentage calculation again to double check that it lined up with the reported percentages (post-interpolation). My intuition was that these steps should be commutative - calculating a percentage and then interpolating should have the same result as interpolating and then calculating the percentage. This was not the case, however - in the variable for population between 25 and 44, the error between these two methods ranged from -12 to 26 percentage points.

```{r, fig.cap = 'Sample error in intensive interpolation', warning=FALSE}
knitr::include_graphics(here('img', 'intensive_error.png'))
```

As it turns out, these steps are not commutative in this way, and the observed error is mostly a function of the weighting between different steps in the process. For example, consider a simple case: suppose source regions A and B are fully contained in target region X, and split X in half. Let the number of people between 25 and 44 in A be 4 (out of 10 total) and in B be 3 (out of 5 total). Taking the percentages first gives us a proportion of 0.4 in A and 0.6 in B. Using the intensive interpolation method on these proportions, both are weighted by the amount of X that the region takes up (half, in each case) and added, so the average weighted by area is 0.5. Conversely, using the extensive interpolation method on the count and popuation, we see that X has 7 people between 25 and 44 (out of 15 total). Taking the percentage, we see that the proportion of this variable in X is ~0.47.

In short, the "calculating proportion" and "areal interpolation" steps are NOT commutative, because of the differences in weights when using intensive vs. extensive interpolation methods. Both are calculating a weighted mean of sorts, but the former is weighting by AREA, while the latter is weighting by POPULATION. In this case we see that the latter is more accurate - source regions with more people should have greater impact on the estimated measures in target regions, because the variables we are dealing with are human-centered rather than space-centered. As such, for this study we will interpolate only the count data (population, numbers of people for each measured variable) and then calculate proportions in the target regions after the interpolation. These proportions are better suited to the regression to ensure that variables are of the same scale and we can compare coefficient estimates.

#### Boundary mismatch

A further issue appears - just like the precinct and census boundaries don't line up because they come from different sources, the outside boundaries of the precinct and census files don't line up. The census boundaries have a lower resolution on the whole than then precinct boundaries. This leads to issues when performing the areal interpolation, because parts of a census boundary that are outside of a precinct will be dropped and cause an underestimate of the true measure of the variables. We address this by bounding both files to only consider the space that is contained in both^[No census tracts were fully removed in this process, but this causes one precinct, Precinct 9900, to be cut off. However, this precinct is a semi-exclave of the county on Alameda Island, across the San Francisco Bay. Since this land, an undeveloped former naval air base, is uninhabited (Levi, 2018) its removal does not impact our results.]. Since we are dealing with a fundamental unit of people instead of space, this ensures that every person is counted, and changes some of the spatial weights calculated. The assumption here is that any region which is only contained in one of the files (precincts without census, or census without precincts) has zero population, because every person should be contained in both a precinct and a census region.

```{r echo = FALSE, fig.width=6.5, fig.cap = 'Boundary intersection between shapefiles'}
knitr::include_graphics(here('img', 'boundary_intersection.png'))
```

This plot^[The plots were clipped to display the same area. There are some extra islands out of range on the precinct and census maps that are not shown.] displays the result of this intersection between the two regions. The impact on the census tracts is quite visible - notably the eastern coastline is much more defined, and the gap in Treasure Island (the large island off the northeastern shore) appears. The impact on the precinct boundaries is less apparent, but still there - foe example, in the southeastern corner it is visible how the angles in the precinct boundary have softened to the more rounded final shape.

Below is a comparison of the original shapefiles^[The original census regions contained the Farallon Islands, which have been removed from this plot because they are uninhabited, 30 miles into the Pacific Ocean, and messed up the scale of the graph.] (including all divisions) to the bounded versions.

```{r fig.cap = 'Original versus bounded shapefiles'}
knitr::include_graphics(here('img', 'clipped_internals.png'))
```

#### Precinct Consolidation

One peculiarity in San Francisco is the combination of certain precincts during elections. California state law allows for counties to "consolidate" precincts with low numbers of registered voters during an election. This eases administration by not requiring counties to set up and staff a full polling place in a precinct with few voters, while still giving voters a physical polling location in their approximate area^[This method of precinct consolidation may change with the 2018 California Voter's Choice Act, which lets counties (except for Los Angeles County) move fully to vote-by-mail in combination with voting centers (California Senate Bill 450).]. While San Francisco does this less often than other counties, there are still some precincts that are consolidated each election.

The areal interpolation process produces demographic estimates for areas in the map, however, which are non-consolidated. This causes a mismatch: we have demographic data for individual precincts, but election data for the consolidated precincts. To address this issue, we have split the election data (a count of overvotes and undervotes) in the consolidated precincts into their two component precincts. This split is weighted by the population of each precinct in 2010^[The most simple weight would be 50-50 for each split, but this is inaccurate for many of these precincts: see the gap in the weights for Pct 7527/7528. The flaw in this method was discovered after calculating turnout of over 1000% for Precinct 7527...]. For example, suppose Precinct X/Y had 100 undervotes, the population of Precinct X in 2010 was 325, and the population of Precinct Y in 2010 was 175. Then the "population split" between X and Y is 65%-35%, and we adjust the undervotes accordingly: Precinct X should have 65 of the 100 undervotes, and Precinct Y should have the remaining 35.

Examples of this in the data are below. The first table is the initial data, with full consolidated counts doubled in the precincts (and thus doubled across rows), and the second table is the data with proper weights applied. Note that this is 6 out of the 12 total precincts that get combined into 6 double precincts.

```{r echo = FALSE}
load(here('data', 'double_precincts.RData'))
original <- doubles_for_table %>%
  filter(type == 'unweighted') %>%
  select(-weight, -type) %>%
  slice(1:6)

knitr::kable(original, 
      col.names = c("Election Pct.", "Pct. Number", "Overvotes", "Not over", "Undervotes", "Not under"),
      caption = "Consolidated Precincts - Original Data",
      caption.short = "Combined Precincts - Original",
      longtable = TRUE,
      booktabs = TRUE)
```

```{r echo = FALSE}
adjusted <- doubles_for_table %>%
  filter(type == 'weighted') %>%
  select(precinct, PREC_2017, weight, everything(), -type) %>%
  slice(1:6)

knitr::kable(adjusted, 
      col.names = c("Election Pct.", "Pct. Number", "Weight", "Overvotes", "Not over", "Undervotes", "Not under"),
      caption = "Consolidated Precincts - Weighted Data",
      caption.short = "Combined Precincts - Weighted",
      longtable = TRUE,
      booktabs = TRUE)
```

## Calculating over/undervote info

Move this further up to the section where you talk about that? Or move that down because it lines up with the actual process

RCV package (with Matthew)
Data from the city elections board
Data is missing some ballots that had to be hand-counted because it spits out straight from the machine, so this won't necessarily match the official reports.

Data comes in in weird double file form, then gets cleaned up and looks like this.

```{r echo = FALSE}
load(here('data', 'sf_ballot.RData'))
image <- sf %>%
  rcv::readable() %>%
  arrange(desc(contest)) %>%
  head(10)

knitr::kable(image, 
      col.names = c("Contest", "Voter ID", "1", "2", "3"),
      caption = "Processed Ballot Image",
      longtable = TRUE,
      booktabs = TRUE)
```

In this view, an overvote in a ranking appears as `NA` in that ranking

```{r}
#mean(sf_no_vote$over) # 0.4% of mayoral voters overvoted, which is (as expected) small
#mean(sf_no_vote$under) # 25% of mayoral voters undervoted, which is pretty large (compare this to other SF elections...?)
```


## Regressions

Methods for regression:

- Linear ?
  + Pretty simple honestly

- Logistic with the "binomial-style" input data
  + This is more useful because our output (rate) is 0-1 limited.
  + It also overweights precincts with more people - is this good? (Ask Heather)

- For whatever goes in, I need to pick a better model selection method.
- Also test / train data set
- Cross-validation?
- ex post facto model validation! The residual plots and all that. This could also go in the results section.
- Weights based on population? Based on number of voters? weights for future research maybe
- surveyglm/svyglm package to do this weighting
- Graphic of precinct turnout
- Regression for voter turnout AT ALL and these demographics
- Histogram just for how much undervoting or overvoting there is
- Zero-inflated? Because there's so many zeroes and we're technically dealing with a census on the voting end (future research, "I'm aware of this")

