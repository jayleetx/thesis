```{r include = FALSE}
library(here)
```

# Background 2: Electric Boogaloo

## Missing data

Many statistical methods require complete data sets, that is data where every possible observation has a proper value, to work correctly (regression, etc.). <!-- find better examples and a source for this --> In many fields of research, however, data collection often results in missing data. Respondents skipping questions (nonresponse), reeponsdents not completing all phases of a multi-stage survey (attrition), governments not reporting certain data, and errors in data collection are all examples of reasons that data can be missing.

Suppose we have a matrix $X$, with some missing data, which has $m$ rows and $n$ columns. Let $Y = (y_{ij})$ be the complete version of dataset $X$, that is every missing data point in $X$ is now observed. Let $A$ be an $m \times n$ matrix, where
\[
A = (a_{ij}) \text{, where } a_{ij} = \cases{1, \text{ when }x_{ij}\text{ is missing}\\
0, \text{ when }x_{ij}\text{ is not missing}}
\]
Let $Y_{obs}$ be the observed data in $Y$, that is the entries in $Y$ corresponding to the entries in $A$ that equal 0, and $Y_{mis}$ be the missing data in $Y$, that is the entries in $Y$ corresponding to the entries in $A$ that equal 1. Let $\phi$ denote some unknown parameters.

The literature on missing data has identified three distinct subtypes of missing data (Rubin & Little, 2002).

- Missing completely at random (MCAR). The distribution of missing values does not depend on any data in $Y$, observed or missing. \[f(M|Y,\phi) = f(M|\phi) \;\forall\; Y,\phi\] An example of this is a survey where the respondent mislabeled a question answer by accident.

- Missing at random (MAR). The distribution of missing values depends on the observed data, but not any missing data. \[f(M|Y,\phi) = f(M|Y_{obs},\phi) \;\forall\; Y_{mis},\phi\] An example of this is an election poll where members of one party are less likely to report their true vote choice than members of another party. The distribution of missing vote choice depends on an observed variable, party identification.

- Non-ignorable (NI), or missing not at random (MNAR). The distribution of missing values depends on the missing values themselves. An example of this is high-income respondents leaving the income field blank in a survey to obscure their true earnings.

## Methods of dealing with missing data

### Listwise deletion

Listwise deletion is the most common^[NB, not the *best*] method of dealing with missing data (source?). In listwise deletion, any row with a missing value is fully removed from the dataset. This can lead to biased parameter estimates when the data is not MCAR. Consider the high-income censoring case as above, with average income being the parameter of interest. If high earners are censoring their income and they are dropped from the dataset, our estimate of average income will be negatively biased.

### Maximum likelihood methods

### Imputation - single and multiple

Unlike maximum likelihood, imputation is a way of filling in the missing data points directly. In single imputation, a single value is estimated for each missing cell in the data. Some variants of single imputation are:

- Mean imputation, where the mean of the observed data column is taken. This can lead to biased estiamtes if the data is not MCAR (the observed data is not representative of the missing data), and also negatively impacts measures of variance.

- Hot deck imputation, where a value from an otherwise similar row in the data is taken.

- Regression imputation, where the missing value is predicted using a regression equation that is trained using the observed data.

- Last value imputation, where the mising value is copied from the immediately preceding observed value. This is particularly common in time series data, where this is a natural ordering of the observations.

Multiple imputation is a method of running single imputation multiple times to create some variability in your estimates. A dataset is imputed multiple times (with some variability between imputations), then for each new dataset the parameter of interest is calculated. This added variability moderates the appearance of "certainty" that comes with single imputation.
